{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/natural-language-processing-spacy-python/#rule-based-matching-using-spacy\n",
    "import spacy #NLP library in python\n",
    "# Load the language model instance in spaCy:\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'tutorial', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'Spacy', '.']\n"
     ]
    }
   ],
   "source": [
    "#create a processed Doc object, which is a container for accessing linguistic annotations, for a given input string:\n",
    "introduction_text = ('This tutorial is about Natural Language Processing in Spacy.')\n",
    "introduction_doc = nlp(introduction_text)\n",
    "# Extract tokens for the given doc\n",
    "print ([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Biden', '’s', 'announcement', 'offered', 'a', 'telling', 'split', '-', 'screen', 'counterpoint', 'to', 'an', 'event', 'being', 'held', 'at', 'the', 'same', 'time', 'at', 'the', 'White', 'House', ':', 'a', 'vaccine', 'summit', 'where', 'President', 'Trump', 'boasted', 'about', 'what', 'he', 'called', 'a', '“', 'monumental', 'national', 'achievement', '”', 'by', 'drug', 'companies', 'to', 'develop', 'a', 'vaccine', 'for', 'the', 'virus', 'in', 'about', 'nine', 'months', '.', 'He', 'did', 'not', 'address', 'the', 'growing', 'death', 'toll', 'or', 'the', 'devastation', 'across', 'the', 'country', ',', 'but', 'he', 'used', 'the', 'occasion', 'to', 'suggest', ',', 'yet', 'again', 'and', 'without', 'evidence', ',', 'that', 'people', 'had', 'tried', 'to', '“', 'steal', '”', 'the', 'election', '.']\n"
     ]
    }
   ],
   "source": [
    "#create a processed Doc object from a text file:\n",
    "file_name = \"Spacy_Data/example.txt\"\n",
    "file_text = open(file_name).read()\n",
    "file_doc = nlp(file_text)\n",
    "# Extract tokens for the given doc\n",
    "print ([token.text for token in file_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Gus Proto is a Python developer currently working for a London-based Fintech company.\n",
      "He is interested in learning Natural Language Processing.\n"
     ]
    }
   ],
   "source": [
    "# Sentence Detection: process of locating the start and end of sentences in a given text; \n",
    "# used in tasks such as part of speech tagging or entity extraction\n",
    "about_text = ('Gus Proto is a Python developer currently' + \n",
    " ' working for a London-based Fintech' + \n",
    " ' company. He is interested in learning'+ \n",
    " ' Natural Language Processing.')\n",
    "about_doc = nlp(about_text)\n",
    "# sents property is used to extract sentences: total number of sentences & sentence itself\n",
    "sentences = list(about_doc.sents)\n",
    "print(len(sentences))\n",
    "for sentence in sentences:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence detection with custom delimiters\n",
      "Gus, can you, ...\n",
      "never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n",
      "sentence detection without custom delimiters to see the difference\n",
      "Gus, can you, ... never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    "# customize the sentence detection to detect sentences on custom delimiters e.g., (...) as the delimeter\n",
    "def set_custom_boundaries(doc):\n",
    "     # Adds support to use `...` as the delimiter for sentence detection\n",
    "     for token in doc[:-1]:\n",
    "         if token.text == '...':\n",
    "             doc[token.i+1].is_sent_start = True\n",
    "     return doc\n",
    "\n",
    "ellipsis_text = ('Gus, can you, ... never mind, I forgot' +\n",
    " ' what I was saying. So, do you think' + \n",
    " ' we should ...')\n",
    "# Load a new model instance\n",
    "custom_nlp = spacy.load('en_core_web_sm')\n",
    "custom_nlp.add_pipe(set_custom_boundaries, before='parser')\n",
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    "# sents property is used to extract sentences: total number of sentences & sentence itself\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    "print(\"sentence detection with custom delimiters\")\n",
    "for sentence in custom_ellipsis_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "print(\"sentence detection without custom delimiters to see the difference\")\n",
    "ellipsis_doc = nlp(ellipsis_text)\n",
    "ellipsis_sentences = list(ellipsis_doc.sents)\n",
    "for sentence in ellipsis_sentences:\n",
    "    print(sentence)\n",
    "\n",
    "#custom_ellipsis_sentences contain three sentences, whereas ellipsis_sentences contains two sentences. \n",
    "# These sentences are still obtained via the sents attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "Python 15\n",
      "developer 22\n",
      "currently 32\n",
      "working 42\n",
      "for 50\n",
      "a 54\n",
      "London 56\n",
      "- 62\n",
      "based 63\n",
      "Fintech 69\n",
      "company 77\n",
      ". 84\n",
      "He 86\n",
      "is 89\n",
      "interested 92\n",
      "in 103\n",
      "learning 106\n",
      "Natural 115\n",
      "Language 123\n",
      "Processing 132\n",
      ". 142\n"
     ]
    }
   ],
   "source": [
    "# Tokenization in spacy: next step after sentence detection -- identifies the basic units in text: tokens\n",
    "# usage: used for further analysis i.e., part of speech tagging\n",
    "# print tokens by iterating on the Doc object:(token, starting index): useful for in-place word replacement\n",
    "for token in about_doc:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus 0 Gus  True False False Xxx False\n",
      "Proto 4 Proto  True False False Xxxxx False\n",
      "is 10 is  True False False xx True\n",
      "a 13 a  True False False x True\n",
      "Python 15 Python  True False False Xxxxx False\n",
      "developer 22 developer  True False False xxxx False\n",
      "currently 32 currently  True False False xxxx False\n",
      "working 42 working  True False False xxxx False\n",
      "for 50 for  True False False xxx True\n",
      "a 54 a  True False False x True\n",
      "London 56 London True False False Xxxxx False\n",
      "- 62 - False True False - False\n",
      "based 63 based  True False False xxxx False\n",
      "Fintech 69 Fintech  True False False Xxxxx False\n",
      "company 77 company True False False xxxx False\n",
      ". 84 .  False True False . False\n",
      "He 86 He  True False False Xx True\n",
      "is 89 is  True False False xx True\n",
      "interested 92 interested  True False False xxxx False\n",
      "in 103 in  True False False xx True\n",
      "learning 106 learning  True False False xxxx False\n",
      "Natural 115 Natural  True False False Xxxxx False\n",
      "Language 123 Language  True False False Xxxxx False\n",
      "Processing 132 Processing True False False Xxxxx False\n",
      ". 142 . False True False . False\n"
     ]
    }
   ],
   "source": [
    "# other attributes\n",
    "# text_with_ws prints token text with trailing space (if present).\n",
    "# is_alpha detects if the token consists of alphabetic characters or not.\n",
    "# is_punct detects if the token is a punctuation symbol or not.\n",
    "# is_space detects if the token is a space or not.\n",
    "# shape_ prints out the shape of the word.\n",
    "# is_stop detects if the token is a stop word or not.\n",
    "\n",
    "for token in about_doc:\n",
    "    print (token, token.idx, token.text_with_ws,\n",
    "        token.is_alpha, token.is_punct, token.is_space,\n",
    "        token.shape_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gus', 'Proto', 'is', 'a', 'Python', 'developer', 'currently', 'working', 'for', 'a', 'London', '-', 'based', 'Fintech', 'company', '.', 'He', 'is', 'interested', 'in', 'learning', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "#customize tokenization process to detect tokens on custom characters e.g., hyphenated words London-based\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
    "infix_re = re.compile(r'''[-~]''')\n",
    "\n",
    "# In order for you to customize, you can pass various parameters to the Tokenizer class:\n",
    "# nlp.vocab is a storage container for special cases and is used to handle cases like contractions and emoticons.\n",
    "# prefix_search is the function that is used to handle preceding punctuation, such as opening parentheses.\n",
    "# infix_finditer is the function that is used to handle non-whitespace separators, such as hyphens.\n",
    "# suffix_search is the function that is used to handle succeeding punctuation, such as closing parentheses.\n",
    "# token_match is an optional boolean function that is used to match strings that should never be split. \n",
    "# It overrides the previous rules and is useful for entities like URLs or numbers.\n",
    "def customize_tokenizer(nlp):\n",
    "    # Adds support to use `-` as the delimiter for tokenization\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "         suffix_search=suffix_re.search,\n",
    "         infix_finditer=infix_re.finditer,\n",
    "         token_match=None)\n",
    "\n",
    "custom_nlp.tokenizer = customize_tokenizer(custom_nlp)\n",
    "custom_tokenizer_about_doc = custom_nlp(about_text)\n",
    "print([token.text for token in custom_tokenizer_about_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "every\n",
      "else\n",
      "did\n",
      "‘m\n",
      "any\n",
      "another\n",
      "however\n",
      "often\n",
      "anyone\n",
      "whenever\n",
      "Gus\n",
      "Proto\n",
      "Python\n",
      "developer\n",
      "currently\n",
      "working\n",
      "London\n",
      "-\n",
      "based\n",
      "Fintech\n",
      "company\n",
      ".\n",
      "interested\n",
      "learning\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      ".\n",
      "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n",
      "Gus Gus\n",
      "is be\n",
      "helping help\n",
      "organize organize\n",
      "a a\n",
      "developerconference developerconference\n",
      "on on\n",
      "Applications Applications\n",
      "of of\n",
      "Natural Natural\n",
      "Language Language\n",
      "Processing Processing\n",
      ". .\n",
      "He -PRON-\n",
      "keeps keep\n",
      "organizing organize\n",
      "local local\n",
      "Python Python\n",
      "meetups meetup\n",
      "and and\n",
      "several several\n",
      "internal internal\n",
      "talks talk\n",
      "at at\n",
      "his -PRON-\n",
      "workplace workplace\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "# spaCy has a list of stop words for the English language:\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)\n",
    "\n",
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "     print(stop_word)\n",
    "        \n",
    "#remove stop words from the input text:\n",
    "for token in about_doc:\n",
    "    if not token.is_stop:\n",
    "        print (token)\n",
    "\n",
    "#create a list of tokens not containing stop words:\n",
    "about_no_stopword_doc = [token for token in about_doc if not token.is_stop]\n",
    "print (about_no_stopword_doc)\n",
    "\n",
    "#lemmatization: process of reducing words to its original form\n",
    "conference_help_text = ('Gus is helping organize a developer'+\n",
    "'conference on Applications of Natural Language'+\n",
    "' Processing. He keeps organizing local Python meetups'+\n",
    "' and several internal talks at his workplace.')\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "for token in conference_help_doc:\n",
    "    print (token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 common Words with their frequenc\n",
      "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n",
      "\n",
      "\n",
      "Unique Words\n",
      "['Proto', 'currentlyworking', 'based', 'company', 'interested', 'conference', 'happening', '21', 'July', '2019', 'titled', 'Applications', 'helpline', 'number', 'available', '+1', '1234567891', 'helping', 'organize', 'keeps', 'organizing', 'local', 'meetups', 'internal', 'talks', 'workplace', 'presenting', 'introduce', 'reader', 'Use', 'cases', 'Apart', 'work', 'passionate', 'music', 'play', 'enrolled', 'weekend', 'batch', 'situated', 'Mayfair', 'City', 'world', 'class', 'piano', 'instructors']\n",
      "\n",
      "\n",
      "example why removing stop word is important\n",
      "[('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)]\n"
     ]
    }
   ],
   "source": [
    "#word Frequency using Spacy\n",
    "from collections import Counter\n",
    "complete_text = ('Gus Proto is a Python developer currently'\n",
    "    'working for a London-based Fintech company. He is'\n",
    "    ' interested in learning Natural Language Processing.'\n",
    "    ' There is a developer conference happening on 21 July'\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number '\n",
    "    ' available at +1-1234567891. Gus is helping organize it.'\n",
    "    ' He keeps organizing local Python meetups and several'\n",
    "    ' internal talks at his workplace. Gus is also presenting'\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    ' Apart from his work, he is very passionate about music.'\n",
    "    ' Gus is learning to play the Piano. He has enrolled '\n",
    "    ' himself in the weekend batch of Great Piano Academy.'\n",
    "    ' Great Piano Academy is situated in Mayfair or the City'\n",
    "    ' of London and has world-class piano instructors.')\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in complete_doc\n",
    "         if not token.is_stop and not token.is_punct]\n",
    "\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "print(\"Top 5 common Words with their frequenc\")\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Unique words\n",
    "print(\"Unique Words\")\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)\n",
    "print(\"\\n\")\n",
    "\n",
    "#example why removing stop word is important\n",
    "print(\"example why removing stop word is important\")\n",
    "words_all = [token.text for token in complete_doc if not token.is_punct]\n",
    "word_freq_all = Counter(words_all)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words_all = word_freq_all.most_common(5)\n",
    "print (common_words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus NNP PROPN noun, proper singular\n",
      "Proto NNP PROPN noun, proper singular\n",
      "is VBZ AUX verb, 3rd person singular present\n",
      "a DT DET determiner\n",
      "Python NNP PROPN noun, proper singular\n",
      "developer NN NOUN noun, singular or mass\n",
      "currently RB ADV adverb\n",
      "working VBG VERB verb, gerund or present participle\n",
      "for IN ADP conjunction, subordinating or preposition\n",
      "a DT DET determiner\n",
      "London NNP PROPN noun, proper singular\n",
      "- HYPH PUNCT punctuation mark, hyphen\n",
      "based VBN VERB verb, past participle\n",
      "Fintech NNP PROPN noun, proper singular\n",
      "company NN NOUN noun, singular or mass\n",
      ". . PUNCT punctuation mark, sentence closer\n",
      "He PRP PRON pronoun, personal\n",
      "is VBZ AUX verb, 3rd person singular present\n",
      "interested JJ ADJ adjective\n",
      "in IN ADP conjunction, subordinating or preposition\n",
      "learning VBG VERB verb, gerund or present participle\n",
      "Natural NNP PROPN noun, proper singular\n",
      "Language NNP PROPN noun, proper singular\n",
      "Processing NNP PROPN noun, proper singular\n",
      ". . PUNCT punctuation mark, sentence closer\n",
      "[developer, company]\n",
      "[interested]\n"
     ]
    }
   ],
   "source": [
    "#parts of speech tagging\n",
    "for token in about_doc:\n",
    "    print (token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "    \n",
    "#tag_ lists the fine-grained part of speech.\n",
    "#pos_ lists the coarse-grained part of speech.\n",
    "#spacy.explain gives descriptive details about a particular POS tag. \n",
    "\n",
    "nouns = []\n",
    "adjectives = []\n",
    "for token in about_doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "print(nouns)\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isa14/opt/anaconda3/envs/ubicos/lib/python3.7/runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"0c3899582bc1491db4a59766e51ad313-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">interested</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Processing.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0c3899582bc1491db4a59766e51ad313-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0c3899582bc1491db4a59766e51ad313-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0c3899582bc1491db4a59766e51ad313-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0c3899582bc1491db4a59766e51ad313-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0c3899582bc1491db4a59766e51ad313-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0c3899582bc1491db4a59766e51ad313-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0c3899582bc1491db4a59766e51ad313-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0c3899582bc1491db4a59766e51ad313-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0c3899582bc1491db4a59766e51ad313-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0c3899582bc1491db4a59766e51ad313-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0c3899582bc1491db4a59766e51ad313-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0c3899582bc1491db4a59766e51ad313-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0c3899582bc1491db4a59766e51ad313-0-6\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0c3899582bc1491db4a59766e51ad313-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "from spacy import displacy\n",
    "about_interest_text = ('He is interested in learning'+\n",
    "    ' Natural Language Processing.')\n",
    "about_interest_doc = nlp(about_interest_text)\n",
    "displacy.serve(about_interest_doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstration of a preprocessing function\n",
    "#preprocessing function that takes text as input and applies the following operations:\n",
    "\n",
    "# Lowercases the text\n",
    "# Lemmatizes each token\n",
    "# Removes punctuation symbols\n",
    "# Removes stop words\n",
    "def is_token_allowed(token):\n",
    "    '''\n",
    "        Only allow valid tokens which are not stop words\n",
    "        and punctuation symbols.\n",
    "    '''\n",
    "    if (not token or not token.string.strip() or\n",
    "        token.is_stop or token.is_punct):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "#if token is allowed, then sent it to preprocess method and add it to the list\n",
    "complete_filtered_tokens = [preprocess_token(token)\n",
    "    for token in complete_doc if is_token_allowed(token)]\n",
    "\n",
    "print(complete_filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns ::  [unit, rate, ratio, denominator, quantities, example, poind]\n",
      "adjectives ::  [different]\n",
      "pronouns ::  [it]\n",
      "conjunction ::  [so]\n"
     ]
    }
   ],
   "source": [
    "about_text = ('a unit rate is a ratio where the denominator is basically it compares different quantities so for an example 2.99/1 poind')\n",
    "about_doc = nlp(about_text)\n",
    "\n",
    "    \n",
    "# #parts of speech tagging\n",
    "# for token in about_doc:\n",
    "#     print (token, token.tag_, token.pos_, spacy.explain(token.tag_))\n",
    "    \n",
    "#tag_ lists the fine-grained part of speech.\n",
    "#pos_ lists the coarse-grained part of speech.\n",
    "#spacy.explain gives descriptive details about a particular POS tag. \n",
    "\n",
    "nouns = []\n",
    "adjectives = []\n",
    "pron = []\n",
    "conjunction = []\n",
    "for token in about_doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "        nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "        adjectives.append(token)\n",
    "    if token.pos_ == 'PRON':\n",
    "        pron.append(token)\n",
    "    if token.pos_ == 'SCONJ':\n",
    "        conjunction.append(token)\n",
    "print('nouns :: ', nouns)\n",
    "print('adjectives :: ',adjectives)\n",
    "print('pronouns :: ',pron)\n",
    "print('conjunction :: ',conjunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
